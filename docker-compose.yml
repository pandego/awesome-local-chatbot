services:

  # --- LLM endpoint --- #
  ollama:
    image: ollama/ollama:${OLLAMA_VERSION}  # or ':latest' -> updated on 2024-04-17
    container_name: ollama
    # pull_policy: always
    tty: true
    volumes:
      - ${OLLAMA_DATA_DIR}:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    restart: always
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
    # Nvidia GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              device_ids: [ '1' ] 
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:11434"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
  
  # --- Vision-LLM endpoint --- #
  automatic1111:
    build: 
      context: .
      dockerfile: Dockerfile.automatic1111
      args:
        AUTOMATIC1111_VERSION: ${AUTOMATIC1111_VERSION}
    container_name: automatic1111
    # network_mode: host
    restart: always
    ports:
      - "${AUTOMATIC1111_PORT}:7860"
    user: "webui"
    volumes:
      - ${AUTOMATIC1111_MODEL_DIR}:/stablediff-web/models/Stable-diffusion
      - ${AUTOMATIC1111_OUTPUT_DIR}:/app/output
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - HUGGING_FACE_HUB_TOKEN=""
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              device_ids: [ '0' ]

    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:7860"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s

  # --- Frontend --- #
  open-webui:
    image: ghcr.io/open-webui/open-webui:${OPEN_WEBUI_VERSION}  # :latest
    container_name: open-webui
    volumes:
      - ${OPEN_WEBUI_DATA_DIR}:/app/backend/data
    # command: ["cp", "/app/backend/data/custom_logo_favicon.png", "/app/backend/static/favicon.png"]
    ports:
      - "${OPEN_WEBUI_PORT}:8080"
    restart: always
    environment:
      - WEBUI_NAME=${WEBUI_NAME}
      - CUSTOM_NAME=${CUSTOM_NAME}
      - SCARF_NO_ANALYTICS=${SCARF_NO_ANALYTICS}
      - DO_NOT_TRACK=${DO_NOT_TRACK}
      - OLLAMA_BASE_URL=http://ollama:${OLLAMA_PORT}
      - AUTOMATIC1111_BASE_URL=http://automatic1111:${AUTOMATIC1111_PORT}
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8080"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
